\documentclass{mg2}
\usepackage{thmbox}
\begin{document}
\newtheorem[S]{definition}{Definition}[subsection]
\newtheorem[S]{lemma}{Lemma}[subsection]
\newtheorem[S]{proposition}{Proposition}[subsection]
\newtheorem[S]{satz}{Satz}[subsection]
\newtheorem[S]{beobachtung}{Beobachtung}[subsection]

\title{Zusammenfassung: Mathematische Grundlagen 2}

\date{SoSe 2014}
\maketitle

\newpage

\tableofcontents

\newpage

\section{Lineare Algebra}
\subsection{Lineare Algebra}
\begin{definition}[Vektor] 
Ein \underline{n-Tupel} reeller Zahlen ist eine geordnete Ansammlung reeller Zahlen. Die Menge aller solcher Tupel bezeichnen wir als $\mathbb{R}^n$. Die Elemente von $\mathbb{R}^n$ werden auch \underline{Vektoren} genannt. 

In $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} \in \mathbb{R}^n$ nennen wir $x_1,\dots,x_n$ die n-te Komponente.
\end{definition}

Die \underline{Summe} von Vektoren in $\mathbb{R}^n$ ist $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} + \begin{pmatrix}y_1\\ \vdots \\ y_n\end{pmatrix}=\begin{pmatrix}x_1 + y_1\\ \vdots \\ x_n + y_n\end{pmatrix}$.

$\mathbb{R}^n + \mathbb{R}^m$ ist nicht definiert! Die Differenz berechnet sich analog.

Der Summenvektor kann als die Diagonale des von beiden ursprünglichen Vektoren aufgespannten Parallelogramms veranschaulicht werden (in $\mathbb{R}^2$).

\begin{definition}[Skalarmultiplikation]Für eine Zahl $c \in \mathbb{R}$ und $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix}$ setze $c \cdot \begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} =\begin{pmatrix}c \cdot x_1\\ \vdots \\c \cdot  x_n\end{pmatrix}$.
\end{definition}

\begin{definition}[Linearkombination, Span]
Für Vektoren $u_1, u_2,\dots,u_k \in \mathbb{R}^n$ und $\lambda_1,\lambda_2,\dots,\lambda_k \in \mathbb{R}$ nennen wir $\lambda_1 u_1, \lambda_2 u_2,\dots,\lambda_k u_k$ die \underline{Linearkombination} von $u_1, u_2,\dots,u_k$.

Der \underline{Span} (lineare Hülle) von $u_1, u_2, \dots, u_k$ ist $Span(u_1, u_2, \dots, u_k) \\
= \{ \lambda_1 u_1 + \lambda_2 u_2 + \dots + \lambda_k u_k | \lambda_1, \dots, \lambda_k \in \mathbb{R} \} \subseteq \mathbb{R}^n$.
\end{definition}

\begin{definition}[lineare (Un)abhängigkeit]
Die Vektoren $v_1, \dots, v_k \in \mathbb{R}^n$ heißen \underline{linear unabhängig}, falls für jede Linearkombination $\lambda_1 v_1, \dots, \lambda_k v_k = 0$ schon gelten muss, dass $\lambda_1 = \dots = \lambda_k = 0$.

Andernfalls heißen $v_1,\dots,v_k$ \underline{linear abhängig}.
\end{definition}

\begin{beobachtung} 
In $\mathbb{R}^n$ sind die Vektoren $v_1,\dots,v_k$ mit $k > n$ immer \textbf{linear abhängig}.
\end{beobachtung}

\begin{lemma}
Ist $w \in Span(u_1,\dots,u_k)$, so ist die Darstellung als Linearkombination von $u_1,\dots,u_k$ genau dann eindeutig, wenn $u_1,\dots,u_k$ linear unabhängig sind.
\end{lemma}

\newpage
\subsection{Unterräume}
\begin{definition}[Unterraum]
Sei $U \subseteq \mathbb{R}^n$ eine nichtleere Teilmenge von $\mathbb{R}^n$.

$U$ heißt Unterraum, falls
\begin{enumerate}
\item für alle Vektoren $u,w \in U$ auch $u+w \in U$ gilt.
\item für alle $u \in U, \lambda \in \mathbb{R}$ ist auch $\lambda \cdot u \in U$.
\end{enumerate}
\end{definition}

Der trivialste Unterraum ist der Nullvektor. $\mathbb{R}^n$ ist immer auch ein Unterraum von $\mathbb{R}^n$.

\begin{proposition}
Sind $u_1,\dots,u_k \in \mathbb{R}^n$, so ist $Span(u_1,\dots,u_k)$ ein Unterraum von $\mathbb{R}^n$.
\end{proposition}

\begin{definition}[Erzeugendensystem, Basis]
Sei $U \subseteq \mathbb{R}^n, u \neq \{0\}$.

Dann nennt man Vektoren $v_1,\dots, v_k \in U$ ein \underline{Erzeugendensystem} von $U$, falls $U = Span(v_1,\dots,v_k)$.

Ist $v_1,\dots,v_k$ ein Ergzeugendensystem von $U$ und zusätzlich linear unabhängig, dann nennt man $v_1,\dots,v_k$ eine \underline{Basis} von $U$.
\end{definition}

Ein Erzeugendensystem muss nicht eindeutig sein.

In $\mathbb{R}^n$ bilden die Vektoren $e_1, \dots, e_n$ die \textbf{Standardbasis} von $\mathbb{R}^n$.

Jeder Unterraum $U \neq \{0\}$ hat eine Basis und alle Basen von $U$ haben die gleiche Anzahl von Elementen.

In einem Unterraum $U \subseteq \mathbb{R}^n, U \neq \{0\}$ kann es höchstens $n$ Vektoren geben, denn in $\mathbb{R}^n$ können höchstens $n$ Vektoren linear unabhängig sein.

\begin{lemma}
Sind Vektoren $u_1,\dots,u_k \in U$, $U$ ein Unterraum von $\mathbb{R}^n$, dann ist $Span(u_1,\dots,u_k)$ eine Teilmenge von $U$.
\end{lemma}

Aussagen über Basen:
\begin{itemize}
\item Sei $U \in \mathbb{R}^n$ ein Unterraum und $U \neq \{0\}$. Dann sind maximal unabhängige Systeme in $U$ dasselbe wie Basen in $U$.
\item Jeder Unterraum von $\mathbb{R}^n$ hat eine Basis.
\item Alle Basen von $U$ haben die gleiche Anzahl von Elementen. Diese Anzahl ist die \underline{Dimension} (dim) von $U$.
\item Die Dimension $dim$ von $\mathbb{R}^n$ ist gleich n.
\item Basen von einem Unterraum $U \neq \{0\}$ von $\mathbb{R}^n$ sind dasselbe wie minimale Erzeugendensysteme von $U$.
\item Ist $U = Span(u_1,\dots,u_k) \subseteq \mathbb{R}^n$, so sind $u_1\dots,u_k$ ein Erzeugendensystem von $U$.
\item Wenn $u_1, \dots, u_k$ linear unabhängig sind, bilden sie eine Basis von $U$.
\item Jede maximale linear unabhängige Auswahl von Vektoren aus $u_1,\dots,u_k$ bildet eine Basis von $U$. Die maximale Anzahl ist genau die Anzahl von Stufen in der Zeilenstufenform (Gauß) der Matrix mit den Spalten $u_1,\dots,u_k$.
\end{itemize}

\begin{beobachtung}
Seien $U \subseteq V \subseteq \mathbb{R}^n$ Unterräume, $U~und~V \neq \{0\}$.

Dann ist $dim~U \leq dim~V$.

Ferner gilt: falls $U \neq V$, dann ist $dim~U < dim~V$. Insbesondere ist $dim~U \leq n$.
\end{beobachtung}

\begin{definition}[Koordinaten bzgl einer Basis]
Sei $U \subseteq \mathbb{R}^n, U \neq \{0\}$ ein Unterraum. Sei $u_1,\dots,u_k$ eine Basis von $U$.

Da $U = Span(u_1,\dots,u_k)$ ist, lässt sich jeder Vektor $w \in U$ eindeutig als Linearkombination von $u_1,\dots,u_k$ schreiben.

Es gibt also Zahlen $\lambda_1,\dots,\lambda_k \in \mathbb{R}$ mit $w = \lambda_1 u_1,\dots,\lambda_k u_k$.

Dann nennen wir $\lambda_1,\dots,\lambda_k$ die \underline{Koordinaten} von $w$ bezüglich der Basis $u_1,\dots,u_k$.

 \underline{k-dimensionaler Koordinatenvektor} von $w = \begin{pmatrix}\lambda_1\\\vdots\\\lambda_k\end{pmatrix}$.
\end{definition}

\subsection{Lineare Abbildungen}
\begin{definition}[Lineare Abbildung]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^l$ Unterräume.

Eine Abbildung $f: U \to V$ heißt \underline{linear}, wenn
\begin{enumerate}
\item Für alle Vektoren $u_1,u_2 \in U$ gilt: $f(u_1+u_2) = f(u_1) + f(u_2)$.
\item Für alle $u \in U, c \in \mathbb{R}$ gilt: $f(u\cdot c) = c \cdot f(u)$.
\end{enumerate}
\end{definition}

\begin{satz}
Sei eine $n \times k$-Matrix vorgegeben. Dann können wir die Abbildung 

$T_A: \mathbb{R}^k \to \mathbb{R}^k; \begin{pmatrix}x_1\\\vdots\\x_k\end{pmatrix} \mapsto A \cdot \begin{pmatrix}x_1\\\vdots\\x_k\end{pmatrix}$ definieren.

Dann ist $T_A$ eine lineare Abbildung.
\end{satz}

\begin{satz}
$T_B: \mathbb{R}^k \to \mathbb{R}^n; x \mapsto B \cdot x$ ist eine lineare Abbildung.
\end{satz}

\begin{proposition}
Sei $B$ eine $n \times k$-Matrix:

$B = \begin{pmatrix} b_{11} \dots b_{1k}\\ b_{21} \dots b_{2k}\\\vdots\\ b_{n1} \dots b_{nk}\end{pmatrix}$ und sei $T_B = \mathbb{R}^k \to \mathbb{R}^n$ die von $B$ dargestellte Abbildung. 

Das Bild $T_B(e_i)$m ist durch die i-te Spalte von $B$ gegeben, also $T_B(e_i) = \begin{pmatrix}b_{1i}\\b_{2i}\\\vdots\\b_{ni}\end{pmatrix}$
\end{proposition}

\begin{satz}[Eindeutigkeit von linearen Abbildungen]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^m$ Unterräume und sei $u_1,\dots,u_k$ eine Basis von $U$.

Seien $v_1,\dots,v_k$ Vektoren in $V$. Dann gibt es genau eine lineare Abbildung $\varphi: U \to V$ mit der Eigenschaft $\varphi(u_i) = v_i$ mit $i = \{1,\dots,k\}$.
\end{satz}

\begin{beobachtung}
Sei $V \subseteq \mathbb{R}^m$ ein Unterraum und seien $v_1,\dots,v_k \in V$. Dann gibt es eine Abbildung $\psi: \mathbb{R}^k \to V$ mit der Eigenschaft $\psi(e_i) = v_i$.

Ist $V = \mathbb{R}^m$, so ist $\psi$ die von der Matrix mit den Spalten $v_1,\dots,v_k$ dargestellte Abbildung, die eindeutig ist.
\end{beobachtung}

\begin{satz}
Seien $U \subseteq \mathbb{R}^m, V \subseteq \mathbb{R}^n$ Unterräume, $u_1,\dots,u_k$ eine Basis von $U$, $v_1,\dots,v_k$ beliebige Vektoren in $V$.

Dann gibt es genau eine lineare Abbildung $g: U \to V$ mit $g(U_j) = V_j$ für alle $1 \leq j \leq k$.

\underline{Daraus folgt}: Jede lineare Abbildung $\varphi: \mathbb{R}^k \to \mathbb{R}^n$ ist durch eie Matrix dargestellt.
\end{satz}

\begin{satz}[Verkettung von linearen Abbildungen]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^m, W \subseteq \mathbb{R}^e$ Unterräume und $f: U \to V$ und $g: V \to W$ lineare Abbildungen.

Dann ist $g \circ f$ auch eine lineare Abbildung: $g \circ f: U \to W$ mit $u \mapsto g(f(u)) (\in V)$.
\end{satz}

\begin{satz}[Kern einer Abbildung]
Seien $U \subseteq \mathbb{R}^m, V \subseteq \mathbb{R}^n$ Unterräume und $g: U \to V$ eine lineare Abbildung. Man nennt die Menge $\{u \in U | g(u) = 0\}$ den \textbf{Kern} der Abbildung und schreibt ker($g$).
\end{satz}

\begin{proposition}
\begin{enumerate}
\item Sei $A$ eine $k \times n$-Matrix. Dann ist die Lösungsmenge vom Gleichungssystem $A \cdot x = 0$ ein Unterraum von $\mathbb{R}^n$, also $\{ x \in \mathbb{R}^n | A \cdot x = 0 \}$.

Ist $T_A : \mathbb{R}^n \to \mathbb{R}^k$ mit $y \mapsto A \cdot y$ die Abbildung mit darstellender Matrix $A$, dann ist $\{x \in \mathbb{R}^n | A \cdot x = 0\} = ker(T_A)$.
\item Ist $A$ eine $k \times n$-Matrix, $b \in \mathbb{R}^k, b \neq 0$. Dann ist $\{ x \in \mathbb{R}^k | A \cdot x = b \}$ kein Unterraum von $\mathbb{R}^n$ (affin), aber: Ist $\{x \in \mathbb{R}^n | A \cdot x = b \} \neq 0$, dann gilt für $y \in \{x \in \mathbb{R}^n | A \cdot x = b\}$: Alle anderen Lösungen entstehen als Summe von $y$ und einem Element von $\{w \in \mathbb{R}^n | A \cdot w = 0\}$ (Unterraum).
\end{enumerate}
\end{proposition}

\begin{definition}[Inverse]
Ist $A$ eine quadratische Matrix $k \times k$, dann nennt man eine Matrix $B$ ($k \times k$) die \underline{Inverse} von $A$, falls $A \cdot B = E_k$ und $B \cdot A = E_k$. Nicht jede quadratische Matrix hat eine Inverse.
\end{definition}

\begin{satz}
Hat eine $n \times n$-Matrix $D$ eine Basis von $\mathbb{R}^n$ als Spalte, so hat $D$ eine inverse Matrix.
\end{satz}
Andersrum: Hat eine Matrix $E$ eine inverse Matrix, so sind ihre Spalten eine Basis von $\mathbb{R}^n$.

\newpage
\subsection{Skalarprodukt, Abstände und Winkel}

\begin{definition}[Skalarprodukt]
Das Standardskalarprodukt auf $\mathbb{R}^n$ ist gegeben durch 

$\left\langle \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix},\begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix} \right\rangle = (x_1\cdot y_1) + (x_2\cdot y_2) + \dots + (x_n\cdot y_n) \in \mathbb{R}^n$.
\end{definition}

\begin{beobachtung}
$\left\langle \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix},\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} \right\rangle = x_1^2 + x_2^2 + \dots + x_n^2 \geq 0$.
\end{beobachtung}

\begin{definition}[Länge eines Vektors]
Die \underline{Länge} eines Vektors $x \in \mathbb{R}^n$ ist gegeben durch

$\left\| x \right\| = \sqrt{\langle x^2,x^2 \rangle}$
\end{definition}

\begin{definition}[Orthogonalität]
Zwei Vektoren $x,y \in \mathbb{R}^n$ heißen \underline{dorthogonal} zueinander, falls das Skalarprodukt null ist.
\end{definition}

\begin{satz}[Eigenschaften vom Skalarprodukt]
\begin{enumerate}
\item Das Skalarprodukt ist symmetrisch: Sind $x,y \in \mathbb{R}^n$, so ist $\langle x,y \rangle = \langle y,x \rangle$.
\item Ist $w \in \mathbb{R}^n$ fest, so ist $\mathbb{R}^n \to \mathbb{R}$ mit $x \mapsto \langle x,y \rangle$ linear, das heißt:
\begin{enumerate}
\item $\langle w, x+y \rangle = \langle w,x \rangle + \langle w,y \rangle \forall x,y \in \mathbb{R}^n$.
\item $\langle w, \lambda \cdot x \rangle = \lambda \cdot \langle w,x \rangle \forall \lambda \in \mathbb{R}, x \in \mathbb{R}^n$.
\end{enumerate}
\item Für $ x \in \mathbb{R}^n$ gilt: $x = 0 \Leftrightarrow \langle x,x \rangle = 0$
\end{enumerate}
\end{satz}
Die darstellende Matrix von $\mathbb{R}^n \to \mathbb{R}$ mit $x \mapsto \langle w,x \rangle$ ist der Vektor $w$ in eine Zeile geschrieben.

\begin{definition}[Orthogonales System]
Die Vektoren $v_1, \dots, v_k \in \mathbb{R}^n$ heißen \underline{orthogonales System}, falls sie paarweise zueinander orthogonal sind, d.h. alle Skalarprodukte $\langle v_i,v_j \rangle = 0$ für alle $1 \leq i \neq j \leq k$.

Eine Basis $w_1,\dots,w_n$ von $\mathbb{R}^n$, die gleichzeitig ein orthogonales System ist, heißt \underline{Orthogonalbasis}.
\end{definition}

\begin{satz}[Orthogonalbasis]
\begin{enumerate}
\item Jeder Unterraum $U \subseteq \mathbb{R}^n, U \neq \{0\}$ hat eine Orthogonalbasis.
\item Ist $v_1, \dots, v_n \in \mathbb{R}^n$ eine Orthogonalbasis und sei $B$ die Matrix mit Spalten $v_1, \dots, v_k$. Dann ist $v_1^T$ der Vektor $v_1$ als Zeile aufgeschrieben.
\end{enumerate}
\end{satz}

\begin{proposition}
Sind $v_1, \dots, v_k \in \mathbb{R}^n$, die ein orthogonales System bilden und gilt $v_I \neq 0$ für $1 \leq i \leq k$, dann sind die Vektoren linear unabhängig.
\end{proposition}

\begin{definition}[Orthogonales Komplement]
Sei $w \in \mathbb{R}^n$, dann  ist $w \perp = \{x \in \mathbb{R}^n |  \langle x,w \rangle  = 0 \}$. Sei ferner $U \subseteq \mathbb{R}^n$ ein Unterraum. Dann nennt man 

$U \perp = \{ x \in \mathbb{R}^n | \forall u \in U: \langle x,u \rangle = 0 \}$ das \underline{orthogonale Komplement} von $U$.
\end{definition}

\begin{satz}
Sei $U \subseteq \mathbb{R}^n$, dim U = $k$. Dann ist $U \perp$ ebenfalls ein Unterraum von $\mathbb{R}^n$ mit $dim(U \perp) = n-k$.

Auch für $w \in \mathbb{R}^n, w \neq 0$ ist $w \perp$ ein (n-1) dimensionaler Unterraum von $\mathbb{R}^n$.
\end{satz}

\begin{beobachtung}
Im Allgemeinen erhalten lineare Abbildungen keine Abstände und rechte Winkel.
\end{beobachtung}

\begin{definition}[Orthogonale lineare Abbildungen]
Eine lineare Abbildung $\varphi: \mathbb{R}^n \to \mathbb{R}^n$ heißt \underline{orthogonal}, falls sie Längen von Vektoren und rechte Winkel zwischen Vektoren erhält, das heißt:
\begin{enumerate}
\item $\left\| u \right\| = \left\| \varphi(u) \right\|$ für alle $u \in \mathbb{R}^n$.
\item Sind $u, v \in \mathbb{R}^n$ mit $\langle u,v \rangle = 0$, so sind ihre Bilder auch orthogonal: $\langle \varphi(u), \varphi(v) \rangle = 0$.
\end{enumerate}
\end{definition}

Die Fläche eines Parallelogramms lässt sich mit der Formel $|ad-bc|$ berechnen. Wir wissen, dass $\begin{pmatrix}a\\b\end{pmatrix}, \begin{pmatrix}c\\d\end{pmatrix}$ genau dann linear unabhängig sind, wenn $ad-bc \neq 0$ ist.
\section{Differential-und Integralrechnung}

\begin{definition}[Grenzwert]
Eine reelle Zahl $a$ ist der \underline{Grenzwert} von einer Folge $(a_n), n \in \mathbb{N}$, wenn es zu jeder Zahl $10^{-m}, m \in \mathbb{N}$ eine natürliche Zahl $N(m)$ gibt, sodass $|a_n - a | \le 10^{-m}$ für $ n \ge N(m).$ Man sagt auch $(a_n)$ konvergiert gegen $a$.
\end{definition}
Für eine irrationale Zahl $b$ und $a \geq 0$ beliebig schreiben wir hier $(b_n)$  für die Dezimaldarstellung der ersten $n$ Stellen von $b$.

Dann konvergiert $(a^{b_n})_{n \geq 0}$ und den Grenzwert bezeichnen wir mit $a^b$.

\begin{definition}[Reihe]
Eine \underline{Reihe} ist eine Folge, deren Folgenglieder selbst Summen von Folgengliedern einer anderen Folge sind.
\end{definition}

\begin{definition}[Ableitung]
Die \underline{Ableitung} einer Funktion $f: I \to \mathbb{R}, I \subseteq \mathbb{R}$ an der Stelle $x = a \in I$ ist definiert als $f'(a) = \lim\limits_{n \rightarrow 0}{\frac{f(a+h)-f(a)}{h}}$ falls der Grenzwert existiert. Falls er nicht existiert, ist die Ableitung nicht definiert.
\end{definition}

\underline{Ableitungsregeln:}\\[1em]
$f,g: I \to \mathbb{R}$ sind differenzierbar, haben also eine Ableitung in jedem Punkt von $I$.

$f+g: I \to \mathbb{R}$ mit $x \mapsto f(x) + g(x)$.
\begin{itemize}
\item \textbf{Summenregel}: $(f+g)' = f' + g'$, d.h. $(f+g)' = f'(x_0) + g'(x_0)$ für alle $x_0 \in I$.
\item \textbf{Produkt} mit einer Konstanten $c \in \mathbb{R}: c \cdot f: I \to \mathbb{R}$ mit $x \mapsto c \cdot f(x)$.

	$(c\cdot f)' = c \cdot f'$
\item \textbf{Produktregel} $(f\cdot g)' = f' \cdot g + f \cdot g'$
\item \textbf{Quotientenregel} $(\frac{f}{g})' = \frac{f' \cdot g - f \cdot g'}{g^2}$
	
	ist nur definiert, wenn $g \neq 0$ für alle $x \in I$.
\item \textbf{Kettenregel} Ist $f \circ g$ definiert, so ist $(f \circ g)'(x) = f'(g(x)) \cdot g'(x)$
\end{itemize}

\begin{definition}[Logarithmus]
Der \underline{Logarithmus} ist die eindeutige Zahl von $c$ zu Basis $a$, die der Gleichung $a^b = c$ genügt (für positive $a$ und $c$).
\end{definition}

\begin{definition}[Eulersche Zahl $e$]
Die \underline{Eulersche Zahl e} ist die eindeutige positive Zahl, so dass die Funktion $\mathbb{R} \to \mathbb{R}$ mit $x \mapsto e^x$ als Ableitungsfunktion $x \mapsto e^x$ hat.

Konventionen: $log_e = ln$, $log_{10} = log$, $log_a (x) = \frac{ln(x)}{ln(a)}$.
\end{definition}

\begin{definition}[Globales Maximum/Minimum]
Sei $f: I \to \mathbb{R}$ eine Funktion. $f$ hat ein
\begin{itemize}
\item (globales) \underline{Maximum} in $x_0 \in I$, falls für alle $x \in I$ gilt: $f(x_0) \geq f(x)$.
\item (globales) \underline{Minimum} in $a \in I$, falls für alle $x \in I$ gilt: $f(a) \leq f(x)$.
\end{itemize}
\end{definition}

\begin{definition}[Lokales Maximum/Minimum]
Ein \underline{lokales Maximum} von $f$ liegt in $x_0$ vor, falls $x_0$ ein globales Maximum auf einem (kleinen) Intervall in $I$ um $x_0 \in I$ ist.

Das \underline{lokale Minimum} ist analog.
\end{definition}

Weiter am 26.06.
\section{Numerik}

\end{document}