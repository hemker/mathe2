\documentclass{mg2}
\usepackage{thmbox}
\begin{document}
\newtheorem[S]{definition}{Definition}[subsection]
\newtheorem[S]{lemma}{Lemma}[subsection]
\newtheorem[S]{proposition}{Proposition}[subsection]
\newtheorem[S]{satz}{Satz}[subsection]
\newtheorem[S]{beobachtung}{Beobachtung}[subsection]

\title{Zusammenfassung: Mathematische Grundlagen 2}

\date{SoSe 2014}
\maketitle

\newpage

\tableofcontents

\newpage

\section{Lineare Algebra}
\subsection{Lineare Algebra}
\begin{definition}[Vektor] 
Ein \underline{n-Tupel} reeller Zahlen ist eine geordnete Ansammlung reeller Zahlen. Die Menge aller solcher Tupel bezeichnen wir als $\mathbb{R}^n$. Die Elemente von $\mathbb{R}^n$ werden auch \underline{Vektoren} genannt. 

In $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} \in \mathbb{R}^n$ nennen wir $x_1,\dots,x_n$ die n-te Komponente.
\end{definition}

Die \underline{Summe} von Vektoren in $\mathbb{R}^n$ ist $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} + \begin{pmatrix}y_1\\ \vdots \\ y_n\end{pmatrix}=\begin{pmatrix}x_1 + y_1\\ \vdots \\ x_n + y_n\end{pmatrix}$.

$\mathbb{R}^n + \mathbb{R}^m$ ist nicht definiert! Die Differenz berechnet sich analog.

Der Summenvektor kann als die Diagonale des von beiden ursprünglichen Vektoren aufgespannten Parallelogramms veranschaulicht werden (in $\mathbb{R}^2$).

\begin{definition}[Skalarmultiplikation]Für eine Zahl $c \in \mathbb{R}$ und $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix}$ setze $c \cdot \begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} =\begin{pmatrix}c \cdot x_1\\ \vdots \\c \cdot  x_n\end{pmatrix}$.
\end{definition}

\begin{definition}[Linearkombination, Span]
Für Vektoren $u_1, u_2,\dots,u_k \in \mathbb{R}^n$ und $\lambda_1,\lambda_2,\dots,\lambda_k \in \mathbb{R}$ nennen wir $\lambda_1 u_1, \lambda_2 u_2,\dots,\lambda_k u_k$ die \underline{Linearkombination} von $u_1, u_2,\dots,u_k$.

Der \underline{Span} (lineare Hülle) von $u_1, u_2, \dots, u_k$ ist $Span(u_1, u_2, \dots, u_k) \\
= \{ \lambda_1 u_1 + \lambda_2 u_2 + \dots + \lambda_k u_k | \lambda_1, \dots, \lambda_k \in \mathbb{R} \} \subseteq \mathbb{R}^n$.
\end{definition}

\begin{definition}[lineare (Un)abhängigkeit]
Die Vektoren $v_1, \dots, v_k \in \mathbb{R}^n$ heißen \underline{linear unabhängig}, falls für jede Linearkombination $\lambda_1 v_1, \dots, \lambda_k v_k = 0$ schon gelten muss, dass $\lambda_1 = \dots = \lambda_k = 0$.

Andernfalls heißen $v_1,\dots,v_k$ \underline{linear abhängig}.
\end{definition}

\begin{beobachtung} 
In $\mathbb{R}^n$ sind die Vektoren $v_1,\dots,v_k$ mit $k > n$ immer \textbf{linear abhängig}.
\end{beobachtung}

\begin{lemma}
Ist $w \in Span(u_1,\dots,u_k)$, so ist die Darstellung als Linearkombination von $u_1,\dots,u_k$ genau dann eindeutig, wenn $u_1,\dots,u_k$ linear unabhängig sind.
\end{lemma}

\newpage
\subsection{Unterräume}
\begin{definition}[Unterraum]
Sei $U \subseteq \mathbb{R}^n$ eine nichtleere Teilmenge von $\mathbb{R}^n$.

$U$ heißt Unterraum, falls
\begin{enumerate}
\item für alle Vektoren $u,w \in U$ auch $u+w \in U$ gilt.
\item für alle $u \in U, \lambda \in \mathbb{R}$ ist auch $\lambda \cdot u \in U$.
\end{enumerate}
\end{definition}

Der trivialste Unterraum ist der Nullvektor. $\mathbb{R}^n$ ist immer auch ein Unterraum von $\mathbb{R}^n$.

\begin{proposition}
Sind $u_1,\dots,u_k \in \mathbb{R}^n$, so ist $Span(u_1,\dots,u_k)$ ein Unterraum von $\mathbb{R}^n$.
\end{proposition}

\begin{definition}[Erzeugendensystem, Basis]
Sei $U \subseteq \mathbb{R}^n, u \neq \{0\}$.

Dann nennt man Vektoren $v_1,\dots, v_k \in U$ ein \underline{Erzeugendensystem} von $U$, falls $U = Span(v_1,\dots,v_k)$.

Ist $v_1,\dots,v_k$ ein Ergzeugendensystem von $U$ und zusätzlich linear unabhängig, dann nennt man $v_1,\dots,v_k$ eine \underline{Basis} von $U$.
\end{definition}

Ein Erzeugendensystem muss nicht eindeutig sein.

In $\mathbb{R}^n$ bilden die Vektoren $e_1, \dots, e_n$ die \textbf{Standardbasis} von $\mathbb{R}^n$.

Jeder Unterraum $U \neq \{0\}$ hat eine Basis und alle Basen von $U$ haben die gleiche Anzahl von Elementen.

In einem Unterraum $U \subseteq \mathbb{R}^n, U \neq \{0\}$ kann es höchstens $n$ Vektoren geben, denn in $\mathbb{R}^n$ können höchstens $n$ Vektoren linear unabhängig sein.

\begin{lemma}
Sind Vektoren $u_1,\dots,u_k \in U$, $U$ ein Unterraum von $\mathbb{R}^n$, dann ist $Span(u_1,\dots,u_k)$ eine Teilmenge von $U$.
\end{lemma}

Aussagen über Basen:
\begin{itemize}
\item Sei $U \in \mathbb{R}^n$ ein Unterraum und $U \neq \{0\}$. Dann sind maximal unabhängige Systeme in $U$ dasselbe wie Basen in $U$.
\item Jeder Unterraum von $\mathbb{R}^n$ hat eine Basis.
\item Alle Basen von $U$ haben die gleiche Anzahl von Elementen. Diese Anzahl ist die \underline{Dimension} (dim) von $U$.
\item Die Dimension $dim$ von $\mathbb{R}^n$ ist gleich n.
\item Basen von einem Unterraum $U \neq \{0\}$ von $\mathbb{R}^n$ sind dasselbe wie minimale Erzeugendensysteme von $U$.
\item Ist $U = Span(u_1,\dots,u_k) \subseteq \mathbb{R}^n$, so sind $u_1\dots,u_k$ ein Erzeugendensystem von $U$.
\item Wenn $u_1, \dots, u_k$ linear unabhängig sind, bilden sie eine Basis von $U$.
\item Jede maximale linear unabhängige Auswahl von Vektoren aus $u_1,\dots,u_k$ bildet eine Basis von $U$. Die maximale Anzahl ist genau die Anzahl von Stufen in der Zeilenstufenform (Gauß) der Matrix mit den Spalten $u_1,\dots,u_k$.
\end{itemize}

\begin{beobachtung}
Seien $U \subseteq V \subseteq \mathbb{R}^n$ Unterräume, $U~und~V \neq \{0\}$.

Dann ist $dim~U \leq dim~V$.

Ferner gilt: falls $U \neq V$, dann ist $dim~U < dim~V$. Insbesondere ist $dim~U \leq n$.
\end{beobachtung}

\begin{definition}[Koordinaten bzgl einer Basis]
Sei $U \subseteq \mathbb{R}^n, U \neq \{0\}$ ein Unterraum. Sei $u_1,\dots,u_k$ eine Basis von $U$.

Da $U = Span(u_1,\dots,u_k)$ ist, lässt sich jeder Vektor $w \in U$ eindeutig als Linearkombination von $u_1,\dots,u_k$ schreiben.

Es gibt also Zahlen $\lambda_1,\dots,\lambda_k \in \mathbb{R}$ mit $w = \lambda_1 u_1,\dots,\lambda_k u_k$.

Dann nennen wir $\lambda_1,\dots,\lambda_k$ die \underline{Koordinaten} von $w$ bezüglich der Basis $u_1,\dots,u_k$.

 \underline{k-dimensionaler Koordinatenvektor} von $w = \begin{pmatrix}\lambda_1\\\vdots\\\lambda_k\end{pmatrix}$.
\end{definition}

\subsection{Lineare Abbildungen}
\begin{definition}[Lineare Abbildung]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^l$ Unterräume.

Eine Abbildung $f: U \to V$ heißt \underline{linear}, wenn
\begin{enumerate}
\item Für alle Vektoren $u_1,u_2 \in U$ gilt: $f(u_1+u_2) = f(u_1) + f(u_2)$.
\item Für alle $u \in U, c \in \mathbb{R}$ gilt: $f(u\cdot c) = c \cdot f(u)$.
\end{enumerate}
\end{definition}

\begin{satz}
Sei eine $n \times k$-Matrix vorgegeben. Dann können wir die Abbildung 

$T_A: \mathbb{R}^k \to \mathbb{R}^k; \begin{pmatrix}x_1\\\vdots\\x_k\end{pmatrix} \mapsto A \cdot \begin{pmatrix}x_1\\\vdots\\x_k\end{pmatrix}$ definieren.

Dann ist $T_A$ eine lineare Abbildung.
\end{satz}

\begin{satz}
$T_B: \mathbb{R}^k \to \mathbb{R}^n; x \mapsto B \cdot x$ ist eine lineare Abbildung.
\end{satz}

\begin{proposition}
Sei $B$ eine $n \times k$-Matrix:

$B = \begin{pmatrix} b_{11} \dots b_{1k}\\ b_{21} \dots b_{2k}\\\vdots\\ b_{n1} \dots b_{nk}\end{pmatrix}$ und sei $T_B = \mathbb{R}^k \to \mathbb{R}^n$ die von $B$ dargestellte Abbildung. 

Das Bild $T_B(e_i)$m ist durch die i-te Spalte von $B$ gegeben, also $T_B(e_i) = \begin{pmatrix}b_{1i}\\b_{2i}\\\vdots\\b_{ni}\end{pmatrix}$
\end{proposition}

\begin{satz}[Eindeutigkeit von linearen Abbildungen]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^m$ Unterräume und sei $u_1,\dots,u_k$ eine Basis von $U$.

Seien $v_1,\dots,v_k$ Vektoren in $V$. Dann gibt es genau eine lineare Abbildung $\varphi: U \to V$ mit der Eigenschaft $\varphi(u_i) = v_i$ mit $i = \{1,\dots,k\}$.
\end{satz}

\begin{beobachtung}
Sei $V \subseteq \mathbb{R}^m$ ein Unterraum und seien $v_1,\dots,v_k \in V$. Dann gibt es eine Abbildung $\psi: \mathbb{R}^k \to V$ mit der Eigenschaft $\psi(e_i) = v_i$.

Ist $V = \mathbb{R}^m$, so ist $\psi$ die von der Matrix mit den Spalten $v_1,\dots,v_k$ dargestellte Abbildung, die eindeutig ist.
\end{beobachtung}

\begin{satz}
Seien $U \subseteq \mathbb{R}^m, V \subseteq \mathbb{R}^n$ Unterräume, $u_1,\dots,u_k$ eine Basis von $U$, $v_1,\dots,v_k$ beliebige Vektoren in $V$.

Dann gibt es genau eine lineare Abbildung $g: U \to V$ mit $g(U_j) = V_j$ für alle $1 \leq j \leq k$.

\underline{Daraus folgt}: Jede lineare Abbildung $\varphi: \mathbb{R}^k \to \mathbb{R}^n$ ist durch eie Matrix dargestellt.
\end{satz}

\begin{satz}[Verkettung von linearen Abbildungen]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^m, W \subseteq \mathbb{R}^e$ Unterräume und $f: U \to V$ und $g: V \to W$ lineare Abbildungen.

Dann ist $g \circ f$ auch eine lineare Abbildung: $g \circ f: U \to W$ mit $u \mapsto g(f(u)) (\in V)$.
\end{satz}

\begin{satz}[Kern einer Abbildung]
Seien $U \subseteq \mathbb{R}^m, V \subseteq \mathbb{R}^n$ Unterräume und $g: U \to V$ eine lineare Abbildung. Man nennt die Menge $\{u \in U | g(u) = 0\}$ den \textbf{Kern} der Abbildung und schreibt ker($g$).
\end{satz}

\begin{proposition}
\begin{enumerate}
\item Sei $A$ eine $k \times n$-Matrix. Dann ist die Lösungsmenge vom Gleichungssystem $A \cdot x = 0$ ein Unterraum von $\mathbb{R}^n$, also $\{ x \in \mathbb{R}^n | A \cdot x = 0 \}$.

Ist $T_A : \mathbb{R}^n \to \mathbb{R}^k$ mit $y \mapsto A \cdot y$ die Abbildung mit darstellender Matrix $A$, dann ist $\{x \in \mathbb{R}^n | A \cdot x = 0\} = ker(T_A)$.
\item Ist $A$ eine $k \times n$-Matrix, $b \in \mathbb{R}^k, b \neq 0$. Dann ist $\{ x \in \mathbb{R}^k | A \cdot x = b \}$ kein Unterraum von $\mathbb{R}^n$ (affin), aber: Ist $\{x \in \mathbb{R}^n | A \cdot x = b \} \neq 0$, dann gilt für $y \in \{x \in \mathbb{R}^n | A \cdot x = b\}$: Alle anderen Lösungen entstehen als Summe von $y$ und einem Element von $\{w \in \mathbb{R}^n | A \cdot w = 0\}$ (Unterraum).
\end{enumerate}
\end{proposition}

\begin{definition}[Inverse]
Ist $A$ eine quadratische Matrix $k \times k$, dann nennt man eine Matrix $B$ ($k \times k$) die \underline{Inverse} von $A$, falls $A \cdot B = E_k$ und $B \cdot A = E_k$. Nicht jede quadratische Matrix hat eine Inverse.
\end{definition}

\begin{satz}
Hat eine $n \times n$-Matrix $D$ eine Basis von $\mathbb{R}^n$ als Spalte, so hat $D$ eine inverse Matrix.
\end{satz}
Andersrum: Hat eine Matrix $E$ eine inverse Matrix, so sind ihre Spalten eine Basis von $\mathbb{R}^n$.

\newpage
\subsection{Skalarprodukt, Abstände und Winkel}

\begin{definition}[Skalarprodukt]
Das Standardskalarprodukt auf $\mathbb{R}^n$ ist gegeben durch 

$\left\langle \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix},\begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix} \right\rangle = (x_1\cdot y_1) + (x_2\cdot y_2) + \dots + (x_n\cdot y_n) \in \mathbb{R}^n$.
\end{definition}

\begin{beobachtung}
$\left\langle \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix},\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} \right\rangle = x_1^2 + x_2^2 + \dots + x_n^2 \geq 0$.
\end{beobachtung}

\begin{definition}[Länge eines Vektors]
Die \underline{Länge} eines Vektors $x \in \mathbb{R}^n$ ist gegeben durch

$\left\| x \right\| = \sqrt{\langle x^2,x^2 \rangle}$
\end{definition}

\begin{definition}[Orthogonalität]
Zwei Vektoren $x,y \in \mathbb{R}^n$ heißen \underline{dorthogonal} zueinander, falls das Skalarprodukt null ist.
\end{definition}

\begin{satz}[Eigenschaften vom Skalarprodukt]
\begin{enumerate}
\item Das Skalarprodukt ist symmetrisch: Sind $x,y \in \mathbb{R}^n$, so ist $\langle x,y \rangle = \langle y,x \rangle$.
\item Ist $w \in \mathbb{R}^n$ fest, so ist $\mathbb{R}^n \to \mathbb{R}$ mit $x \mapsto \langle x,y \rangle$ linear, das heißt:
\begin{enumerate}
\item $\langle w, x+y \rangle = \langle w,x \rangle + \langle w,y \rangle \forall x,y \in \mathbb{R}^n$.
\item $\langle w, \lambda \cdot x \rangle = \lambda \cdot \langle w,x \rangle \forall \lambda \in \mathbb{R}, x \in \mathbb{R}^n$.
\end{enumerate}
\item Für $ x \in \mathbb{R}^n$ gilt: $x = 0 \Leftrightarrow \langle x,x \rangle = 0$
\end{enumerate}
\end{satz}
Die darstellende Matrix von $\mathbb{R}^n \to \mathbb{R}$ mit $x \mapsto \langle w,x \rangle$ ist der Vektor $w$ in eine Zeile geschrieben.

\begin{definition}[Orthogonales System]
Die Vektoren $v_1, \dots, v_k \in \mathbb{R}^n$ heißen \underline{orthogonales System}, falls sie paarweise zueinander orthogonal sind, d.h. alle Skalarprodukte $\langle v_i,v_j \rangle = 0$ für alle $1 \leq i \neq j \leq k$.

Eine Basis $w_1,\dots,w_n$ von $\mathbb{R}^n$, die gleichzeitig ein orthogonales System ist, heißt \underline{Orthogonalbasis}.
\end{definition}

\begin{satz}[Orthogonalbasis]
\begin{enumerate}
\item Jeder Unterraum $U \subseteq \mathbb{R}^n, U \neq \{0\}$ hat eine Orthogonalbasis.
\item Ist $v_1, \dots, v_n \in \mathbb{R}^n$ eine Orthogonalbasis und sei $B$ die Matrix mit Spalten $v_1, \dots, v_k$. Dann ist $v_1^T$ der Vektor $v_1$ als Zeile aufgeschrieben.
\end{enumerate}
\end{satz}

\begin{proposition}
Sind $v_1, \dots, v_k \in \mathbb{R}^n$, die ein orthogonales System bilden und gilt $v_I \neq 0$ für $1 \leq i \leq k$, dann sind die Vektoren linear unabhängig.
\end{proposition}

\begin{definition}[Orthogonales Komplement]
Sei $w \in \mathbb{R}^n$, dann  ist $w \perp = \{x \in \mathbb{R}^n |  \langle x,w \rangle  = 0 \}$. Sei ferner $U \subseteq \mathbb{R}^n$ ein Unterraum. Dann nennt man 

$U \perp = \{ x \in \mathbb{R}^n | \forall u \in U: \langle x,u \rangle = 0 \}$ das \underline{orthogonale Komplement} von $U$.
\end{definition}

\begin{satz}
Sei $U \subseteq \mathbb{R}^n$, dim U = $k$. Dann ist $U \perp$ ebenfalls ein Unterraum von $\mathbb{R}^n$ mit $dim(U \perp) = n-k$.

Auch für $w \in \mathbb{R}^n, w \neq 0$ ist $w \perp$ ein (n-1) dimensionaler Unterraum von $\mathbb{R}^n$.
\end{satz}

\begin{beobachtung}
Im Allgemeinen erhalten lineare Abbildungen keine Abstände und rechte Winkel.
\end{beobachtung}

\begin{definition}[Orthogonale lineare Abbildungen]
Eine lineare Abbildung $\varphi: \mathbb{R}^n \to \mathbb{R}^n$ heißt \underline{orthogonal}, falls sie Längen von Vektoren und rechte Winkel zwischen Vektoren erhält, das heißt:
\begin{enumerate}
\item $\left\| u \right\| = \left\| \varphi(u) \right\|$ für alle $u \in \mathbb{R}^n$.
\item Sind $u, v \in \mathbb{R}^n$ mit $\langle u,v \rangle = 0$, so sind ihre Bilder auch orthogonal: $\langle \varphi(u), \varphi(v) \rangle = 0$.
\end{enumerate}
\end{definition}

Die Fläche eines Parallelogramms lässt sich mit der Formel $|ad-bc|$ berechnen. Wir wissen, dass $\begin{pmatrix}a\\b\end{pmatrix}, \begin{pmatrix}c\\d\end{pmatrix}$ genau dann linear unabhängig sind, wenn $ad-bc \neq 0$ ist.
\section{Differential-und Integralrechnung}

\begin{definition}[Grenzwert]
Eine reelle Zahl $a$ ist der \underline{Grenzwert} von einer Folge $(a_n), n \in \mathbb{N}$, wenn es zu jeder Zahl $10^{-m}, m \in \mathbb{N}$ eine natürliche Zahl $N(m)$ gibt, sodass $|a_n - a | \le 10^{-m}$ für $ n \ge N(m).$ Man sagt auch $(a_n)$ konvergiert gegen $a$.
\end{definition}
Für eine irrationale Zahl $b$ und $a \geq 0$ beliebig schreiben wir hier $(b_n)$  für die Dezimaldarstellung der ersten $n$ Stellen von $b$.

Dann konvergiert $(a^{b_n})_{n \geq 0}$ und den Grenzwert bezeichnen wir mit $a^b$.

\begin{definition}[Reihe]
Eine \underline{Reihe} ist eine Folge, deren Folgenglieder selbst Summen von Folgengliedern einer anderen Folge sind.
\end{definition}

\begin{definition}[Ableitung]
Die \underline{Ableitung} einer Funktion $f: I \to \mathbb{R}, I \subseteq \mathbb{R}$ an der Stelle $x = a \in I$ ist definiert als $f'(a) = \lim\limits_{n \rightarrow 0}{\frac{f(a+h)-f(a)}{h}}$ falls der Grenzwert existiert. Falls er nicht existiert, ist die Ableitung nicht definiert.
\end{definition}

\underline{Ableitungsregeln:}\\[1em]
$f,g: I \to \mathbb{R}$ sind differenzierbar, haben also eine Ableitung in jedem Punkt von $I$.

$f+g: I \to \mathbb{R}$ mit $x \mapsto f(x) + g(x)$.
\begin{itemize}
\item \textbf{Summenregel}: $(f+g)' = f' + g'$, d.h. $(f+g)' = f'(x_0) + g'(x_0)$ für alle $x_0 \in I$.
\item \textbf{Produkt} mit einer Konstanten $c \in \mathbb{R}: c \cdot f: I \to \mathbb{R}$ mit $x \mapsto c \cdot f(x)$.

	$(c\cdot f)' = c \cdot f'$
\item \textbf{Produktregel} $(f\cdot g)' = f' \cdot g + f \cdot g'$
\item \textbf{Quotientenregel} $(\frac{f}{g})' = \frac{f' \cdot g - f \cdot g'}{g^2}$
	
	ist nur definiert, wenn $g \neq 0$ für alle $x \in I$.
\item \textbf{Kettenregel} Ist $f \circ g$ definiert, so ist $(f \circ g)'(x) = f'(g(x)) \cdot g'(x)$
\end{itemize}

\begin{definition}[Logarithmus]
Der \underline{Logarithmus} ist die eindeutige Zahl von $c$ zu Basis $a$, die der Gleichung $a^b = c$ genügt (für positive $a$ und $c$).
\end{definition}

\begin{definition}[Eulersche Zahl $e$]
Die \underline{Eulersche Zahl e} ist die eindeutige positive Zahl, so dass die Funktion $\mathbb{R} \to \mathbb{R}$ mit $x \mapsto e^x$ als Ableitungsfunktion $x \mapsto e^x$ hat.

Konventionen: $log_e = ln$, $log_{10} = log$, $log_a (x) = \frac{ln(x)}{ln(a)}$.
\end{definition}

\begin{definition}[Globales Maximum/Minimum]
Sei $f: I \to \mathbb{R}$ eine Funktion. $f$ hat ein
\begin{itemize}
\item (globales) \underline{Maximum} in $x_0 \in I$, falls für alle $x \in I$ gilt: $f(x_0) \geq f(x)$.
\item (globales) \underline{Minimum} in $a \in I$, falls für alle $x \in I$ gilt: $f(a) \leq f(x)$.
\end{itemize}
\end{definition}

\begin{definition}[Lokales Maximum/Minimum]
Ein \underline{lokales Maximum} von $f$ liegt in $x_0$ vor, falls $x_0$ ein globales Maximum auf einem (kleinen) Intervall in $I$ um $x_0 \in I$ ist.

Das \underline{lokale Minimum} ist analog.
\end{definition}

\underline{Bestimmung von lokalen und globalen Extrema:}\\[1em]
von $g: I \to \mathbb{R}$

\begin{enumerate}
\item Man bestimme $g': I \to \mathbb{R}$.
\item Man bestimme die Nullstelle von $g'$.
\item Man bestimme das Vorzeichenverhalten von $g'$ auf $I$. Liegt in einer Nullstelle $a$ von $g'$ ein Vorzeichenwechsel von + nach - vor, so hat $g$ in $a$ ein lokales Maximum. Umgekert: Von - nach +: lokales Minimum. Liegt kein Vorzeichenwechsel vor, so liegt auch kein Extremum vor (mit kleinen Ausnahmen).
\item Man bestimme die Werte von $g$ in allen lokalen Extrema.
\item Enthält $I$ ein oder mehrere abgeschlossene oder halb offene Intervalle, so berechne man die Werte von $g$ an den Intervallgrenzen.
\item Man vergleiche die Werte aus Schritten 4 und 5 und bestimme dadurch die globalen Extrema.
\end{enumerate}

\begin{definition}[Monotonie]
Eine Funktion $f: I \to \mathbb{R}$ ist \underline{streng monoton steigend}, falls für alle $x_1,x_2 \in I$ mit $x_1 < x_2$ auch $f(x_1) < f(x_2)$ gilt.

Die Funktion heißt \underline{schwach monoton steigend}, falls für alle $x_1,x_2 \in I$ mit $x_1 < x_2$ auch $f(x_1) \leq f(x_2)$ gilt.

Monoton fallend ist analog.
\end{definition}

\begin{definition}
Sei $f: I \to \mathbb{R}$ eine differenzierbare Funktion. Wenn $f$ ein lokales Extremum in $x_0 \in I$ hat, dann ist $f'(x_0) = 0$.

\underline{Warnung:} Wenn $f'(x) = 0$, muss kein lokales Extremum vorliegen.
\end{definition}

\begin{definition}
Sei $f: I \to \mathbb{R}$ eine differenzierbare Funktion. Ist $f$ auf $I$ monoton steigend, dann ist die Ableitung $f'(x) \geq 0$ auf ganz $I$, d.h. für alle $x \in I$.

Ist $f'(x) > 0$ für alle $x \in I$, so ist $f$ monoton steigend auf $I$.

Sei $f: I \to \mathbb{R}$ eine differenzierbare Funktion. Ist $f$ auf $I$ monoton fallend, dann ist die Ableitung $f'(x) \leq 0$ auf ganz $I$, d.h. für alle $x \in I$.

Ist $f'(x) > 0$ für alle $x \in I$, so ist $f$ monoton fallend auf $I$.
\end{definition}

\underline{Additionstheoreme}\\[1em]
$cos(\alpha + \beta) = cos\alpha \cdot cos \beta - sin \alpha \cdot sin \beta$

$sin(\alpha + \beta) = sin \alpha \cdot cos \beta + sin \beta \cdot cos \alpha$

\begin{definition}[Polynom]
Ein \underline{Polynom} in einer Variablen $x$ ist ein Ausdruck der Form $P(x) = a_0 x^0 + a_1 x^1 + a_2 x^2 + ... + a_k x^k$, wobei $a_0,a_1,...,a_k \in \mathbb{R}$ (Koeffizienten).

Ist $a_k \neq 0$, so nennt man $k$ den \underline{Grad} des Polynoms
\end{definition}

\begin{definition}[Glatte Funktionen]
Eine Funktion heißt \underline{glatt}, falls man sie beliebig oft ableiten kann.
\end{definition}

\begin{definition}[Taylor Polynom]
Sei $f$ eine glatte Funktion. Dann nennt man das Polynom 

$(T_n f)(x) = \sum\nolimits_{k=0}^n \frac{f^{(k)} (x_0)}{k!} \cdot (x-x_0)^k$ 

das $n$-te Taylor-Polynom von $f$ an der Stelle $x_0$ ($x_0$ muss im Definitionsbereich liegen).

Konventionen:\\
 $k! = k\cdot(k-1)\cdot(k-1)\cdot... \cdot 1,\\
  0! = 1,\\
   f^{(0)} = f$
\end{definition}

\begin{satz}
Sei $f: I \to \mathbb{R}$ eine glatte Funktion, $x_0 \in I, I = (a,b)$.

$f(x) - (T_n f)(x) = \frac{f^{(n+1)}(c)}{(n+1)!} \cdot (x-x_0)^{n+1}$

und $c$ ist in $I$ zwischen $x$ und $x_0: x_0 \leq c \leq x$ oder $x \leq c \leq x_0$.
\end{satz}

\begin{definition}[Ober-/Untersumme]
Sei $f: [a,b] \to \mathbb{R}$ eine Funktion. Wir betrachten eine \underline{Unterteilung} $a = x_0 < x_1 < x_2 < ... < x_k = b$ des Intervalls [a,b].

\underline{Obersumme}: $O = \sum\nolimits_{i=0}^{k-1} max(f)_{[x_i, x_{i+1}]} \cdot (x_{i+1} - x_i)$

\underline{Untersumme}: $U = \sum\nolimits_{i=0}^{k-1} min(f)_{[x_i, x_{i+1}]} \cdot (x_{i+1} - x_i)$
\end{definition}

\begin{definition}[Integral]
$\int_a^b \! f(x) \, \mathrm{d}x$ = Übereinstimmen des Grenzwertes der Ober- und Untersumme, falls dieser existiert.

Man kann auch (für Approximationen) beliebige Werte der Funktion $f$ auf jedem Intervall $[x_i, x_{i+1}]$ wählen.

\underline{Konvention}: $\int_a^b \! f(x) \, \mathrm{d}x = \int_b^a \! f(x) \, \mathrm{d}x, a < b$
\end{definition}

\begin{satz}[Hauptsatz der Integral- und Differentialrechnung]
Sei $f: [a,b] \to \mathbb{R}$ eine (stetige) Funktion. Sei $c \in [a,b]$ und $F_c: [a,b] \to \mathbb{R}$ mit $x \mapsto \int_a^b \! f(x) \, \mathrm{d}x$.

Dann ist $F_c$ eine \underline{Stammfunktion} von $f$, d.h. $F_c'(x) = f(x)$. Jede andere Stammfunktion von $f$ lässt sich als $F(x) = F_c(x)+d, d \in \mathbb{R}$, schreiben. Stammfunktionen sind nicht eindeutig.

Ist insbesondere $g: [d_1, b_1] \to \mathbb{R}$ eine Funktion, die die Ableitungsfunktion von $G: [a_1, b_1] \to \mathbb{R}$ ist, dann lässt sich $\int_{a_1}^{b_1} \! g(y) \, \mathrm{d}y$ als $G(b) - G(a)$ ausrechnen. Wir schreiben $G |_a^b$ dafür.
\end{satz}

\underline{Einige Stammfunktionen}\\[1em]
 \begin{tabular}{l|l}
\textbf{Funktion} & \textbf{eine Stammfunktion}\\\hline
$x \mapsto x^\alpha$ & $x \mapsto \frac{x^{\alpha+1}}{\alpha+1}, \alpha \neq 0$\\\hline
$x \mapsto \frac{1}{x}, x > 0$ & $x \mapsto ln(x), x > 0$\\\hline
$x \mapsto e^x$ & $x \mapsto e^x$\\\hline
$x \mapsto sin(x)$ & $x \mapsto -cos(x)$\\\hline
$x \mapsto cos(x)$ & $x \mapsto sin(x)$\\\hline
 \end{tabular}
 
 \underline{Integrationsregeln}\\[1em]
 \begin{tabular}{l|p{8cm}}
\textbf{Ableitungsregel} & \textbf{Integrationsregel}\\\hline
Summenregel & $\int_a^b \! (f(x) + g(x)) \, \mathrm{d}x = \int_a^b \! f(x) \, \mathrm{d}x + \int_a^b \! g(x) \, \mathrm{d}x$\\\hline
Produktregel & partielle Integration: $\int_a^b \! (u'(x) \cdot v(x)) \, \mathrm{d}x = u(x)v(x)|_a^b - \int_a^b \! u(x) \cdot v'(x) \, \mathrm{d}x$\\\hline
Kettenregel & Substitutionsregel: $\int_a^b \! f(\varphi(t)) \cdot \varphi'(t) \, \mathrm{d}t = \int_{\varphi(a)}^{\varphi(b)} \! f(x) \, \mathrm{d}x$\\\hline
 \end{tabular}
 
 \begin{satz}[Volumenbestimmung]
 Sei ein 3D-Körper vorgegeben, so dass die Funktion $F: [a,b] \to \mathbb{R}$, die jeder Stelle $x \in [a,b]$ die Querschnittsfläche an der Stelle $x$ zuordnet, integrierbar ist.
 
 Dann ist das Volumen von dem Körper gegeben durch $V = \int_a^b \! F(x) \, \mathrm{d}x$.
 \end{satz}
 
\section{Numerik}
Absoluter Fehler: $x' - x$\\
Relativer Fehler: $\frac{x'-x}{x}$\\
\subsection{Newton-Verfahren}
\begin{satz}
Ist $f$ 3x differenzierbar und $f'(x) \neq 0$ für alle $x \in [a,b], x_0 \in [a,b]$, dann konvergiert die Folge $(x_n)$ gegen eine Nullstelle von $f$ (und alle $x_n \in [a,b]$).
\end{satz}
\subsection{Interpolation}
\begin{satz}[Interpolation]
Seien $(n+1)$ Stützpunkte vorgegeben: $(x_0,y_0), (x_1,y_1), ..., (x_n,y_n)$ (mit $x_i \neq x_j$ für $i \neq j$). Dann gibt es ein Polynom $P$ vom Grad $\leq n$, sodass $P(x_i) = y_i$ für alle $0 \leq i \leq n$.
\end{satz}
\end{document}