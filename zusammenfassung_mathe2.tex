\documentclass{mg2}
\usepackage{hyperref}
\usepackage{thmbox}
\begin{document}
\newtheorem[S]{definition}{Definition}[subsection]
\newtheorem[S]{lemma}{Lemma}[subsection]
\newtheorem[S]{proposition}{Proposition}[subsection]
\newtheorem[S]{satz}{Satz}[subsection]
\newtheorem[S]{beobachtung}{Beobachtung}[subsection]

\title{Mathematische Grundlagen 2}
\subtitle{Lineare Algebra und Differential- und Integralrechnung}
\date{SoSe 2014}
\author{Dozentin: Dr. Viktoriya Ozornova}

\maketitle


\textbf{Hinweis}: Dies ist für die Klausurvorbereitung eine inoffizielle Zusammenfassung der Definitionen, Sätze, Theoreme und Propositionen, die in der Veranstaltung vorgestellt worden. Die Richtigkeit wird nicht garantiert. Ihr dürft euch zur Verbesserung der Zusammenfassung gerne an dem Github-Projekt\footnote{\url{https://github.com/dschwarz90/mathe2}} beteiligen.


\tableofcontents

\newpage

\section{Lineare Algebra}
\subsection{Lineare Algebra}
\begin{definition}[Vektor] 
Ein \underline{n-Tupel} reeller Zahlen ist eine geordnete Ansammlung reeller Zahlen. Die Menge aller solcher Tupel bezeichnen wir als $\mathbb{R}^n$. Die Elemente von $\mathbb{R}^n$ werden auch \underline{Vektoren} genannt. 

In $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} \in \mathbb{R}^n$ nennen wir $x_1,\dots,x_n$ die n-te Komponente.
\end{definition}

Die \underline{Summe} von Vektoren in $\mathbb{R}^n$ ist $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} + \begin{pmatrix}y_1\\ \vdots \\ y_n\end{pmatrix}=\begin{pmatrix}x_1 + y_1\\ \vdots \\ x_n + y_n\end{pmatrix}$.

$\mathbb{R}^n + \mathbb{R}^m$ ist nicht definiert! Die Differenz berechnet sich analog.

Der Summenvektor kann als die Diagonale des von beiden ursprünglichen Vektoren aufgespannten Parallelogramms veranschaulicht werden (in $\mathbb{R}^2$).

\begin{definition}[Skalarmultiplikation]Für eine Zahl $c \in \mathbb{R}$ und $\begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix}$ setze $c \cdot \begin{pmatrix}x_1\\ \vdots \\ x_n\end{pmatrix} =\begin{pmatrix}c \cdot x_1\\ \vdots \\c \cdot  x_n\end{pmatrix}$.
\end{definition}

\begin{definition}[Linearkombination, Span]
Für Vektoren $u_1, u_2,\dots,u_k \in \mathbb{R}^n$ und $\lambda_1,\lambda_2,\dots,\lambda_k \in \mathbb{R}$ nennen wir $\lambda_1 u_1, \lambda_2 u_2,\dots,\lambda_k u_k$ die \underline{Linearkombination} von $u_1, u_2,\dots,u_k$.

Der \underline{Span} (lineare Hülle) von $u_1, u_2, \dots, u_k$ ist $Span(u_1, u_2, \dots, u_k) \\
= \{ \lambda_1 u_1 + \lambda_2 u_2 + \dots + \lambda_k u_k | \lambda_1, \dots, \lambda_k \in \mathbb{R} \} \subseteq \mathbb{R}^n$.
\end{definition}

\begin{definition}[lineare (Un)abhängigkeit]
Die Vektoren $v_1, \dots, v_k \in \mathbb{R}^n$ heißen \underline{linear unabhängig}, falls für jede Linearkombination $\lambda_1 v_1, \dots, \lambda_k v_k = 0$ schon gelten muss, dass $\lambda_1 = \dots = \lambda_k = 0$.

Andernfalls heißen $v_1,\dots,v_k$ \underline{linear abhängig}.
\end{definition}

\begin{beobachtung} 
In $\mathbb{R}^n$ sind die Vektoren $v_1,\dots,v_k$ mit $k > n$ immer \textbf{linear abhängig}.
\end{beobachtung}

\begin{lemma}
Ist $w \in Span(u_1,\dots,u_k)$, so ist die Darstellung als Linearkombination von $u_1,\dots,u_k$ genau dann eindeutig, wenn $u_1,\dots,u_k$ linear unabhängig sind.
\end{lemma}

\newpage
\subsection{Unterräume}
\begin{definition}[Unterraum]
Sei $U \subseteq \mathbb{R}^n$ eine nichtleere Teilmenge von $\mathbb{R}^n$.

$U$ heißt Unterraum, falls
\begin{enumerate}
\item für alle Vektoren $u,w \in U$ auch $u+w \in U$ gilt.
\item für alle $u \in U, \lambda \in \mathbb{R}$ ist auch $\lambda \cdot u \in U$.
\end{enumerate}
\end{definition}

Der trivialste Unterraum ist der Nullvektor. $\mathbb{R}^n$ ist immer auch ein Unterraum von $\mathbb{R}^n$.

\begin{proposition}
Sind $u_1,\dots,u_k \in \mathbb{R}^n$, so ist $Span(u_1,\dots,u_k)$ ein Unterraum von $\mathbb{R}^n$.
\end{proposition}

\begin{definition}[Erzeugendensystem, Basis]
Sei $U \subseteq \mathbb{R}^n, u \neq \{0\}$.

Dann nennt man Vektoren $v_1,\dots, v_k \in U$ ein \underline{Erzeugendensystem} von $U$, falls $U = Span(v_1,\dots,v_k)$.

Ist $v_1,\dots,v_k$ ein Ergzeugendensystem von $U$ und zusätzlich linear unabhängig, dann nennt man $v_1,\dots,v_k$ eine \underline{Basis} von $U$.
\end{definition}

Ein Erzeugendensystem muss nicht eindeutig sein.

In $\mathbb{R}^n$ bilden die Vektoren $e_1, \dots, e_n$ die \textbf{Standardbasis} von $\mathbb{R}^n$.

Jeder Unterraum $U \neq \{0\}$ hat eine Basis und alle Basen von $U$ haben die gleiche Anzahl von Elementen.

In einem Unterraum $U \subseteq \mathbb{R}^n, U \neq \{0\}$ kann es höchstens $n$ Vektoren geben, denn in $\mathbb{R}^n$ können höchstens $n$ Vektoren linear unabhängig sein.

\begin{lemma}
Sind Vektoren $u_1,\dots,u_k \in U$, $U$ ein Unterraum von $\mathbb{R}^n$, dann ist $Span(u_1,\dots,u_k)$ eine Teilmenge von $U$.
\end{lemma}

Aussagen über Basen:
\begin{itemize}
\item Sei $U \in \mathbb{R}^n$ ein Unterraum und $U \neq \{0\}$. Dann sind maximal unabhängige Systeme in $U$ dasselbe wie Basen in $U$.
\item Jeder Unterraum von $\mathbb{R}^n$ hat eine Basis.
\item Alle Basen von $U$ haben die gleiche Anzahl von Elementen. Diese Anzahl ist die \underline{Dimension} (dim) von $U$.
\item Die Dimension $dim$ von $\mathbb{R}^n$ ist gleich n.
\item Basen von einem Unterraum $U \neq \{0\}$ von $\mathbb{R}^n$ sind dasselbe wie minimale Erzeugendensysteme von $U$.
\item Ist $U = Span(u_1,\dots,u_k) \subseteq \mathbb{R}^n$, so sind $u_1\dots,u_k$ ein Erzeugendensystem von $U$.
\item Wenn $u_1, \dots, u_k$ linear unabhängig sind, bilden sie eine Basis von $U$.
\item Jede maximale linear unabhängige Auswahl von Vektoren aus $u_1,\dots,u_k$ bildet eine Basis von $U$. Die maximale Anzahl ist genau die Anzahl von Stufen in der Zeilenstufenform (Gauß) der Matrix mit den Spalten $u_1,\dots,u_k$.
\end{itemize}

\begin{beobachtung}
Seien $U \subseteq V \subseteq \mathbb{R}^n$ Unterräume, $U~und~V \neq \{0\}$.

Dann ist $dim~U \leq dim~V$.

Ferner gilt: falls $U \neq V$, dann ist $dim~U < dim~V$. Insbesondere ist $dim~U \leq n$.
\end{beobachtung}

\begin{definition}[Koordinaten bzgl einer Basis]
Sei $U \subseteq \mathbb{R}^n, U \neq \{0\}$ ein Unterraum. Sei $u_1,\dots,u_k$ eine Basis von $U$.

Da $U = Span(u_1,\dots,u_k)$ ist, lässt sich jeder Vektor $w \in U$ eindeutig als Linearkombination von $u_1,\dots,u_k$ schreiben.

Es gibt also Zahlen $\lambda_1,\dots,\lambda_k \in \mathbb{R}$ mit $w = \lambda_1 u_1,\dots,\lambda_k u_k$.

Dann nennen wir $\lambda_1,\dots,\lambda_k$ die \underline{Koordinaten} von $w$ bezüglich der Basis $u_1,\dots,u_k$.

 \underline{k-dimensionaler Koordinatenvektor} von $w = \begin{pmatrix}\lambda_1\\\vdots\\\lambda_k\end{pmatrix}$.
\end{definition}

\subsection{Lineare Abbildungen}
\begin{definition}[Lineare Abbildung]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^l$ Unterräume.

Eine Abbildung $f: U \to V$ heißt \underline{linear}, wenn
\begin{enumerate}
\item Für alle Vektoren $u_1,u_2 \in U$ gilt: $f(u_1+u_2) = f(u_1) + f(u_2)$.
\item Für alle $u \in U, c \in \mathbb{R}$ gilt: $f(u\cdot c) = c \cdot f(u)$.
\end{enumerate}
\end{definition}

\begin{satz}
Sei eine $n \times k$-Matrix vorgegeben. Dann können wir die Abbildung 

$T_A: \mathbb{R}^k \to \mathbb{R}^k; \begin{pmatrix}x_1\\\vdots\\x_k\end{pmatrix} \mapsto A \cdot \begin{pmatrix}x_1\\\vdots\\x_k\end{pmatrix}$ definieren.

Dann ist $T_A$ eine lineare Abbildung.
\end{satz}

\begin{satz}
$T_B: \mathbb{R}^k \to \mathbb{R}^n; x \mapsto B \cdot x$ ist eine lineare Abbildung.
\end{satz}

\begin{proposition}
Sei $B$ eine $n \times k$-Matrix: $B = \begin{pmatrix} b_{11} \dots b_{1k}\\ b_{21} \dots b_{2k}\\\vdots\\ b_{n1} \dots b_{nk}\end{pmatrix}$ und sei $T_B = \mathbb{R}^k \to \mathbb{R}^n$ die von $B$ dargestellte Abbildung. 

Das Bild $T_B(e_i)$m ist durch die i-te Spalte von $B$ gegeben, also $T_B(e_i) = \begin{pmatrix}b_{1i}\\b_{2i}\\\vdots\\b_{ni}\end{pmatrix}$
\end{proposition}

\begin{satz}[Eindeutigkeit von linearen Abbildungen]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^m$ Unterräume und sei $u_1,\dots,u_k$ eine Basis von $U$.

Seien $v_1,\dots,v_k$ Vektoren in $V$. Dann gibt es genau eine lineare Abbildung $\varphi: U \to V$ mit der Eigenschaft $\varphi(u_i) = v_i$ mit $i = \{1,\dots,k\}$.
\end{satz}

\begin{beobachtung}
Sei $V \subseteq \mathbb{R}^m$ ein Unterraum und seien $v_1,\dots,v_k \in V$. Dann gibt es eine Abbildung $\psi: \mathbb{R}^k \to V$ mit der Eigenschaft $\psi(e_i) = v_i$.

Ist $V = \mathbb{R}^m$, so ist $\psi$ die von der Matrix mit den Spalten $v_1,\dots,v_k$ dargestellte Abbildung, die eindeutig ist.
\end{beobachtung}

\begin{satz}
Seien $U \subseteq \mathbb{R}^m, V \subseteq \mathbb{R}^n$ Unterräume, $u_1,\dots,u_k$ eine Basis von $U$, $v_1,\dots,v_k$ beliebige Vektoren in $V$.

Dann gibt es genau eine lineare Abbildung $g: U \to V$ mit $g(U_j) = V_j$ für alle $1 \leq j \leq k$.

\underline{Daraus folgt}: Jede lineare Abbildung $\varphi: \mathbb{R}^k \to \mathbb{R}^n$ ist durch eie Matrix dargestellt.
\end{satz}

\begin{satz}[Verkettung von linearen Abbildungen]
Seien $U \subseteq \mathbb{R}^n, V \subseteq \mathbb{R}^m, W \subseteq \mathbb{R}^e$ Unterräume und $f: U \to V$ und $g: V \to W$ lineare Abbildungen.

Dann ist $g \circ f$ auch eine lineare Abbildung: $g \circ f: U \to W$ mit $u \mapsto g(f(u)) (\in V)$.
\end{satz}

\begin{satz}[Kern einer Abbildung]
Seien $U \subseteq \mathbb{R}^m, V \subseteq \mathbb{R}^n$ Unterräume und $g: U \to V$ eine lineare Abbildung. Man nennt die Menge $\{u \in U | g(u) = 0\}$ den \textbf{Kern} der Abbildung und schreibt ker($g$).
\end{satz}

\begin{proposition}
\begin{enumerate}
\item Sei $A$ eine $k \times n$-Matrix. Dann ist die Lösungsmenge vom Gleichungssystem $A \cdot x = 0$ ein Unterraum von $\mathbb{R}^n$, also $\{ x \in \mathbb{R}^n | A \cdot x = 0 \}$.

Ist $T_A : \mathbb{R}^n \to \mathbb{R}^k$ mit $y \mapsto A \cdot y$ die Abbildung mit darstellender Matrix $A$, dann ist $\{x \in \mathbb{R}^n | A \cdot x = 0\} = ker(T_A)$.
\item Ist $A$ eine $k \times n$-Matrix, $b \in \mathbb{R}^k, b \neq 0$. Dann ist $\{ x \in \mathbb{R}^k | A \cdot x = b \}$ kein Unterraum von $\mathbb{R}^n$ (affin), aber: Ist $\{x \in \mathbb{R}^n | A \cdot x = b \} \neq 0$, dann gilt für $y \in \{x \in \mathbb{R}^n | A \cdot x = b\}$: Alle anderen Lösungen entstehen als Summe von $y$ und einem Element von $\{w \in \mathbb{R}^n | A \cdot w = 0\}$ (Unterraum).
\end{enumerate}
\end{proposition}

\begin{definition}[Inverse]
Ist $A$ eine quadratische Matrix $k \times k$, dann nennt man eine Matrix $B$ ($k \times k$) die \underline{Inverse} von $A$, falls $A \cdot B = E_k$ und $B \cdot A = E_k$. Nicht jede quadratische Matrix hat eine Inverse.
\end{definition}

\begin{satz}
Hat eine $n \times n$-Matrix $D$ eine Basis von $\mathbb{R}^n$ als Spalte, so hat $D$ eine inverse Matrix.
\end{satz}
Andersrum: Hat eine Matrix $E$ eine inverse Matrix, so sind ihre Spalten eine Basis von $\mathbb{R}^n$.


\subsection{Skalarprodukt, Abstände und Winkel}

\begin{definition}[Skalarprodukt]
Das Standardskalarprodukt auf $\mathbb{R}^n$ ist gegeben durch 

$\left\langle \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix},\begin{pmatrix}y_1\\\vdots\\y_n\end{pmatrix} \right\rangle = (x_1\cdot y_1) + (x_2\cdot y_2) + \dots + (x_n\cdot y_n) \in \mathbb{R}^n$.
\end{definition}

\begin{beobachtung}
$\left\langle \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix},\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} \right\rangle = x_1^2 + x_2^2 + \dots + x_n^2 \geq 0$.
\end{beobachtung}

\begin{definition}[Länge eines Vektors]
Die \underline{Länge} eines Vektors $x \in \mathbb{R}^n$ ist gegeben durch

$\left\| x \right\| = \sqrt{\langle x,x \rangle}$
\end{definition}

\begin{definition}[Orthogonalität]
Zwei Vektoren $x,y \in \mathbb{R}^n$ heißen \underline{orthogonal} zueinander, falls das Skalarprodukt null ist.
\end{definition}

\begin{satz}[Eigenschaften vom Skalarprodukt]
\begin{enumerate}
\item Das Skalarprodukt ist symmetrisch: Sind $x,y \in \mathbb{R}^n$, so ist $\langle x,y \rangle = \langle y,x \rangle$.
\item Ist $w \in \mathbb{R}^n$ fest, so ist $\mathbb{R}^n \to \mathbb{R}$ mit $x \mapsto \langle x,y \rangle$ linear, das heißt:
\begin{enumerate}
\item $\langle w, x+y \rangle = \langle w,x \rangle + \langle w,y \rangle \forall x,y \in \mathbb{R}^n$.
\item $\langle w, \lambda \cdot x \rangle = \lambda \cdot \langle w,x \rangle \forall \lambda \in \mathbb{R}, x \in \mathbb{R}^n$.
\end{enumerate}
\item Für $ x \in \mathbb{R}^n$ gilt: $x = 0 \Leftrightarrow \langle x,x \rangle = 0$
\end{enumerate}
\end{satz}
Die darstellende Matrix von $\mathbb{R}^n \to \mathbb{R}$ mit $x \mapsto \langle w,x \rangle$ ist der Vektor $w$ in eine Zeile geschrieben.

\begin{definition}[Orthogonales System]
Die Vektoren $v_1, \dots, v_k \in \mathbb{R}^n$ heißen \underline{orthogonales System}, falls sie paarweise zueinander orthogonal sind, d.h. alle Skalarprodukte $\langle v_i,v_j \rangle = 0$ für alle $1 \leq i \neq j \leq k$.

Eine Basis $w_1,\dots,w_n$ von $\mathbb{R}^n$, die gleichzeitig ein orthogonales System ist, heißt \underline{Orthogonalbasis}.
\end{definition}

\begin{satz}[Orthogonalbasis]
\begin{enumerate}
\item Jeder Unterraum $U \subseteq \mathbb{R}^n, U \neq \{0\}$ hat eine Orthogonalbasis.
\item Ist $v_1, \dots, v_n \in \mathbb{R}^n$ eine Orthogonalbasis und sei $B$ die Matrix mit Spalten $v_1, \dots, v_k$. Dann ist $v_1^T$ der Vektor $v_1$ als Zeile aufgeschrieben.
\end{enumerate}
\end{satz}

\begin{proposition}
Sind $v_1, \dots, v_k \in \mathbb{R}^n$, die ein orthogonales System bilden und gilt $v_I \neq 0$ für $1 \leq i \leq k$, dann sind die Vektoren linear unabhängig.
\end{proposition}

\begin{definition}[Orthogonales Komplement]
Sei $w \in \mathbb{R}^n$, dann  ist $w \perp = \{x \in \mathbb{R}^n |  \langle x,w \rangle  = 0 \}$. Sei ferner $U \subseteq \mathbb{R}^n$ ein Unterraum. Dann nennt man 

$U \perp = \{ x \in \mathbb{R}^n | \forall u \in U: \langle x,u \rangle = 0 \}$ das \underline{orthogonale Komplement} von $U$.
\end{definition}

\begin{satz}
Sei $U \subseteq \mathbb{R}^n$, dim U = $k$. Dann ist $U \perp$ ebenfalls ein Unterraum von $\mathbb{R}^n$ mit $dim(U \perp) = n-k$.

Auch für $w \in \mathbb{R}^n, w \neq 0$ ist $w \perp$ ein (n-1) dimensionaler Unterraum von $\mathbb{R}^n$.
\end{satz}

\begin{beobachtung}
Im Allgemeinen erhalten lineare Abbildungen keine Abstände und rechte Winkel.
\end{beobachtung}

\begin{definition}[Orthogonale lineare Abbildungen]
Eine lineare Abbildung $\varphi: \mathbb{R}^n \to \mathbb{R}^n$ heißt \underline{orthogonal}, falls sie Längen von Vektoren und rechte Winkel zwischen Vektoren erhält, das heißt:
\begin{enumerate}
\item $\left\| u \right\| = \left\| \varphi(u) \right\|$ für alle $u \in \mathbb{R}^n$.
\item Sind $u, v \in \mathbb{R}^n$ mit $\langle u,v \rangle = 0$, so sind ihre Bilder auch orthogonal: $\langle \varphi(u), \varphi(v) \rangle = 0$.
\end{enumerate}
\end{definition}

Die Fläche eines Parallelogramms lässt sich mit der Formel $|ad-bc|$ berechnen. Wir wissen, dass $\begin{pmatrix}a\\b\end{pmatrix}, \begin{pmatrix}c\\d\end{pmatrix}$ genau dann linear unabhängig sind, wenn $ad-bc \neq 0$ ist.
\section{Differential-und Integralrechnung}

\begin{definition}[Grenzwert]
Eine reelle Zahl $a$ ist der \underline{Grenzwert} von einer Folge $(a_n), n \in \mathbb{N}$, wenn es zu jeder Zahl $10^{-m}, m \in \mathbb{N}$ eine natürliche Zahl $N(m)$ gibt, sodass $|a_n - a | \le 10^{-m}$ für $ n \ge N(m).$ Man sagt auch $(a_n)$ konvergiert gegen $a$.
\end{definition}
Für eine irrationale Zahl $b$ und $a \geq 0$ beliebig schreiben wir hier $(b_n)$  für die Dezimaldarstellung der ersten $n$ Stellen von $b$.

Dann konvergiert $(a^{b_n})_{n \geq 0}$ und den Grenzwert bezeichnen wir mit $a^b$.

\begin{definition}[Reihe]
Eine \underline{Reihe} ist eine Folge, deren Folgenglieder selbst Summen von Folgengliedern einer anderen Folge sind.
\end{definition}

\begin{definition}[Ableitung]
Die \underline{Ableitung} einer Funktion $f: I \to \mathbb{R}, I \subseteq \mathbb{R}$ an der Stelle $x = a \in I$ ist definiert als $f'(a) = \lim\limits_{n \rightarrow 0}{\frac{f(a+h)-f(a)}{h}}$ falls der Grenzwert existiert. Falls er nicht existiert, ist die Ableitung nicht definiert.
\end{definition}

\underline{Ableitungsregeln:}\\[1em]
$f,g: I \to \mathbb{R}$ sind differenzierbar, haben also eine Ableitung in jedem Punkt von $I$.

$f+g: I \to \mathbb{R}$ mit $x \mapsto f(x) + g(x)$.
\begin{itemize}
\item \textbf{Summenregel}: $(f+g)' = f' + g'$, d.h. $(f+g)' = f'(x_0) + g'(x_0)$ für alle $x_0 \in I$.
\item \textbf{Produkt} mit einer Konstanten $c \in \mathbb{R}: c \cdot f: I \to \mathbb{R}$ mit $x \mapsto c \cdot f(x)$.

	$(c\cdot f)' = c \cdot f'$
\item \textbf{Produktregel} $(f\cdot g)' = f' \cdot g + f \cdot g'$
\item \textbf{Quotientenregel} $(\frac{f}{g})' = \frac{f' \cdot g - f \cdot g'}{g^2}$
	
	ist nur definiert, wenn $g \neq 0$ für alle $x \in I$.
\item \textbf{Kettenregel} Ist $f \circ g$ definiert, so ist $(f \circ g)'(x) = f'(g(x)) \cdot g'(x)$
\end{itemize}

\begin{definition}[Logarithmus]
Der \underline{Logarithmus} ist die eindeutige Zahl von $c$ zu Basis $a$, die der Gleichung $a^b = c$ genügt (für positive $a$ und $c$).
\end{definition}

\begin{definition}[Eulersche Zahl $e$]
Die \underline{Eulersche Zahl e} ist die eindeutige positive Zahl, so dass die Funktion $\mathbb{R} \to \mathbb{R}$ mit $x \mapsto e^x$ als Ableitungsfunktion $x \mapsto e^x$ hat.

Konventionen: $log_e = ln$, $log_{10} = log$, $log_a (x) = \frac{ln(x)}{ln(a)}$.
\end{definition}

\begin{definition}[Globales Maximum/Minimum]
Sei $f: I \to \mathbb{R}$ eine Funktion. $f$ hat ein
\begin{itemize}
\item (globales) \underline{Maximum} in $x_0 \in I$, falls für alle $x \in I$ gilt: $f(x_0) \geq f(x)$.
\item (globales) \underline{Minimum} in $a \in I$, falls für alle $x \in I$ gilt: $f(a) \leq f(x)$.
\end{itemize}
\end{definition}

\begin{definition}[Lokales Maximum/Minimum]
Ein \underline{lokales Maximum} von $f$ liegt in $x_0$ vor, falls $x_0$ ein globales Maximum auf einem (kleinen) Intervall in $I$ um $x_0 \in I$ ist.

Das \underline{lokale Minimum} ist analog.
\end{definition}

\underline{Bestimmung von lokalen und globalen Extrema:}\\[1em]
von $g: I \to \mathbb{R}$

\begin{enumerate}
\item Man bestimme $g': I \to \mathbb{R}$.
\item Man bestimme die Nullstelle von $g'$.
\item Man bestimme das Vorzeichenverhalten von $g'$ auf $I$. Liegt in einer Nullstelle $a$ von $g'$ ein Vorzeichenwechsel von + nach - vor, so hat $g$ in $a$ ein lokales Maximum. Umgekert: Von - nach +: lokales Minimum. Liegt kein Vorzeichenwechsel vor, so liegt auch kein Extremum vor (mit kleinen Ausnahmen).
\item Man bestimme die Werte von $g$ in allen lokalen Extrema.
\item Enthält $I$ ein oder mehrere abgeschlossene oder halb offene Intervalle, so berechne man die Werte von $g$ an den Intervallgrenzen.
\item Man vergleiche die Werte aus Schritten 4 und 5 und bestimme dadurch die globalen Extrema.
\end{enumerate}

\begin{definition}[Monotonie]
Eine Funktion $f: I \to \mathbb{R}$ ist \underline{streng monoton steigend}, falls für alle $x_1,x_2 \in I$ mit $x_1 < x_2$ auch $f(x_1) < f(x_2)$ gilt.

Die Funktion heißt \underline{schwach monoton steigend}, falls für alle $x_1,x_2 \in I$ mit $x_1 < x_2$ auch $f(x_1) \leq f(x_2)$ gilt.

Monoton fallend ist analog.
\end{definition}

\begin{definition}
Sei $f: I \to \mathbb{R}$ eine differenzierbare Funktion. Wenn $f$ ein lokales Extremum in $x_0 \in I$ hat, dann ist $f'(x_0) = 0$.

\underline{Warnung:} Wenn $f'(x) = 0$, muss kein lokales Extremum vorliegen.
\end{definition}

\begin{definition}
Sei $f: I \to \mathbb{R}$ eine differenzierbare Funktion. Ist $f$ auf $I$ monoton steigend, dann ist die Ableitung $f'(x) \geq 0$ auf ganz $I$, d.h. für alle $x \in I$.

Ist $f'(x) > 0$ für alle $x \in I$, so ist $f$ monoton steigend auf $I$.

Sei $f: I \to \mathbb{R}$ eine differenzierbare Funktion. Ist $f$ auf $I$ monoton fallend, dann ist die Ableitung $f'(x) \leq 0$ auf ganz $I$, d.h. für alle $x \in I$.

Ist $f'(x) > 0$ für alle $x \in I$, so ist $f$ monoton fallend auf $I$.
\end{definition}

\underline{Additionstheoreme}\\[1em]
$cos(\alpha + \beta) = cos\alpha \cdot cos \beta - sin \alpha \cdot sin \beta$

$sin(\alpha + \beta) = sin \alpha \cdot cos \beta + sin \beta \cdot cos \alpha$

\begin{definition}[Polynom]
Ein \underline{Polynom} in einer Variablen $x$ ist ein Ausdruck der Form $P(x) = a_0 x^0 + a_1 x^1 + a_2 x^2 + ... + a_k x^k$, wobei $a_0,a_1,...,a_k \in \mathbb{R}$ (Koeffizienten).

Ist $a_k \neq 0$, so nennt man $k$ den \underline{Grad} des Polynoms
\end{definition}

\begin{definition}[Glatte Funktionen]
Eine Funktion heißt \underline{glatt}, falls man sie beliebig oft ableiten kann.
\end{definition}

\begin{definition}[Taylor Polynom]
Sei $f$ eine glatte Funktion. Dann nennt man das Polynom 

$(T_n f)(x) = \sum\nolimits_{k=0}^n \frac{f^{(k)} (x_0)}{k!} \cdot (x-x_0)^k$ 

das $n$-te Taylor-Polynom von $f$ an der Stelle $x_0$ ($x_0$ muss im Definitionsbereich liegen).

Konventionen:\\
 $k! = k\cdot(k-1)\cdot(k-1)\cdot... \cdot 1,\\
  0! = 1,\\
   f^{(0)} = f$
\end{definition}

\begin{satz}
Sei $f: I \to \mathbb{R}$ eine glatte Funktion, $x_0 \in I, I = (a,b)$.

$f(x) - (T_n f)(x) = \frac{f^{(n+1)}(c)}{(n+1)!} \cdot (x-x_0)^{n+1}$

und $c$ ist in $I$ zwischen $x$ und $x_0: x_0 \leq c \leq x$ oder $x \leq c \leq x_0$.
\end{satz}

\begin{definition}[Ober-/Untersumme]
Sei $f: [a,b] \to \mathbb{R}$ eine Funktion. Wir betrachten eine \underline{Unterteilung} $a = x_0 < x_1 < x_2 < ... < x_k = b$ des Intervalls [a,b].

\underline{Obersumme}: $O = \sum\nolimits_{i=0}^{k-1} max(f)_{[x_i, x_{i+1}]} \cdot (x_{i+1} - x_i)$

\underline{Untersumme}: $U = \sum\nolimits_{i=0}^{k-1} min(f)_{[x_i, x_{i+1}]} \cdot (x_{i+1} - x_i)$
\end{definition}

\begin{definition}[Integral]
$\int_a^b \! f(x) \, \mathrm{d}x$ = Übereinstimmen des Grenzwertes der Ober- und Untersumme, falls dieser existiert.

Man kann auch (für Approximationen) beliebige Werte der Funktion $f$ auf jedem Intervall $[x_i, x_{i+1}]$ wählen.

\underline{Konvention}: $\int_a^b \! f(x) \, \mathrm{d}x = \int_b^a \! f(x) \, \mathrm{d}x, a < b$
\end{definition}

\begin{satz}[Hauptsatz der Integral- und Differentialrechnung]
Sei $f: [a,b] \to \mathbb{R}$ eine (stetige) Funktion. Sei $c \in [a,b]$ und $F_c: [a,b] \to \mathbb{R}$ mit $x \mapsto \int_a^b \! f(x) \, \mathrm{d}x$.

Dann ist $F_c$ eine \underline{Stammfunktion} von $f$, d.h. $F_c'(x) = f(x)$. Jede andere Stammfunktion von $f$ lässt sich als $F(x) = F_c(x)+d, d \in \mathbb{R}$, schreiben. Stammfunktionen sind nicht eindeutig.

Ist insbesondere $g: [d_1, b_1] \to \mathbb{R}$ eine Funktion, die die Ableitungsfunktion von $G: [a_1, b_1] \to \mathbb{R}$ ist, dann lässt sich $\int_{a_1}^{b_1} \! g(y) \, \mathrm{d}y$ als $G(b) - G(a)$ ausrechnen. Wir schreiben $G |_a^b$ dafür.
\end{satz}

\newpage

\underline{Einige Stammfunktionen}\\[1em]
 \begin{tabular}{l|l}
\textbf{Funktion} & \textbf{eine Stammfunktion}\\\hline
$x \mapsto x^\alpha$ & $x \mapsto \frac{x^{\alpha+1}}{\alpha+1}, \alpha \neq 0$\\\hline
$x \mapsto \frac{1}{x}, x > 0$ & $x \mapsto ln(x), x > 0$\\\hline
$x \mapsto e^x$ & $x \mapsto e^x$\\\hline
$x \mapsto sin(x)$ & $x \mapsto -cos(x)$\\\hline
$x \mapsto cos(x)$ & $x \mapsto sin(x)$\\\hline
 \end{tabular}
 
 \underline{Integrationsregeln}\\[1em]
 \begin{tabular}{l|p{8cm}}
\textbf{Ableitungsregel} & \textbf{Integrationsregel}\\\hline
Summenregel & $\int_a^b \! (f(x) + g(x)) \, \mathrm{d}x = \int_a^b \! f(x) \, \mathrm{d}x + \int_a^b \! g(x) \, \mathrm{d}x$\\\hline
Produktregel & partielle Integration: $\int_a^b \! (u'(x) \cdot v(x)) \, \mathrm{d}x = u(x)v(x)|_a^b - \int_a^b \! u(x) \cdot v'(x) \, \mathrm{d}x$\\\hline
Kettenregel & Substitutionsregel: $\int_a^b \! f(\varphi(t)) \cdot \varphi'(t) \, \mathrm{d}t = \int_{\varphi(a)}^{\varphi(b)} \! f(x) \, \mathrm{d}x$\\\hline
 \end{tabular}
 
 \begin{satz}[Volumenbestimmung]
 Sei ein 3D-Körper vorgegeben, so dass die Funktion $F: [a,b] \to \mathbb{R}$, die jeder Stelle $x \in [a,b]$ die Querschnittsfläche an der Stelle $x$ zuordnet, integrierbar ist.
 
 Dann ist das Volumen von dem Körper gegeben durch $V = \int_a^b \! F(x) \, \mathrm{d}x$.
 \end{satz}
 
\section{Numerik}
Absoluter Fehler: $x' - x$\\
Relativer Fehler: $\frac{x'-x}{x}$\\
\subsection{Newton-Verfahren}
\begin{satz}
Ist $f$ 3x differenzierbar und $f'(x) \neq 0$ für alle $x \in [a,b], x_0 \in [a,b]$, dann konvergiert die Folge $(x_n)$ gegen eine Nullstelle von $f$ (und alle $x_n \in [a,b]$).
\end{satz}
\subsection{Interpolation}
\begin{satz}[Interpolation]
Seien $(n+1)$ Stützpunkte vorgegeben: $(x_0,y_0), (x_1,y_1), ..., (x_n,y_n)$ (mit $x_i \neq x_j$ für $i \neq j$). Dann gibt es ein Polynom $P$ vom Grad $\leq n$, sodass $P(x_i) = y_i$ für alle $0 \leq i \leq n$.
\end{satz}
\end{document}